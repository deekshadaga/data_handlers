---
 -  hosts: localhost
    become: yes
    become_method: sudo
    tasks:
    - name: adding ansiblle repo
      yum_repository:
        name: ansible
        description: ansible yum repo
        baseurl: http://13.126.248.209/summer19/ansible27/
        gpgcheck: no
    - name: install ansible
      yum:
        name: ansible
        enablerepo: ansible
        state: present
    - name: Unarchive hadoop file
      unarchive: 
        src: http://13.234.66.67/summer19/bigdata/hadoop-2.7.3.tar.gz
        dest: /home/ec2-user
        remote_src: yes
        creates: /home/ec2-user/hadoop-2.7.3  
    - name: install java 
      yum: 
        name: http://monalisa.cern.ch/MONALISA/download/java/jdk-8-linux-x64.rpm
        state: present
    - name: entry in host file
      lineinfile:
        path: /etc/hosts
        line: 172.31.11.108  ec2-13-234-189-125.ap-south-1.compute.amazonaws.com
    - name: entry in hdfs site.xml
      blockinfile:
        path: /home/ec2-user/hadoop-2.7.3/etc/hadoop/hdfs-site.xml
        insertafter: '<configuration>'
        block: |
            <property>
            <name>dfs.namenode.name.dir</name>
            <value>/nn</value>
            </property>
            <property>
            <name>dfs.datanode.data.dir</name>
            <value>/dn</value>
            </property>
    - name: entry in core-site.xml
      blockinfile:
        path: /home/ec2-user/hadoop-2.7.3/etc/hadoop/core-site.xml 
        insertafter: '<configuration>'
        block: |
            <property>
            <name>fs.defaultFS</name>
            <value>hdfs://13.234.189.125:10002</value>
            </property>
    - name: entry in mapred-site.xml
      blockinfile:
        path: /home/ec2-user/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template
        insertafter: '<configuration>'
        block: |
            <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
            </property>
    - name: entry in yarn-site.xml
      blockinfile:
        path: /home/ec2-user/hadoop-2.7.3/etc/hadoop/yarn-site.xml
        insertafter: '<configuration>'
        block: |
            <property>
            <name>yarn.resourcemanager.resource-tracker.address</name>
            <value>ec2-13-234-189-125.ap-south-1.compute.amazonaws.com:8025</value>
            </property>

            <property>
            <name>yarn.resourcemanager.scheduler.address</name>
            <value>ec2-13-234-189-125.ap-south-1.compute.amazonaws.com:8030</value>
            </property>
 
            <property>
            <name>yarn.resourcemanager.address</name>
            <value>ec2-13-234-189-125.ap-south-1.compute.amazonaws.com:8032</value>
            </property>

            <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
            </property>
    - name: Unarchive spark file
      unarchive:
        src: http://13.234.66.67/summer19/bigdata/spark-2.4.0-bin-hadoop2.7.tgz
        dest: /home/ec2-user
        remote_src: yes
        creates: /home/ec2-user/spark-2.4.0-bin-hadoop2.7
    - name: Unarchive hive file
      unarchive:
        src: http://mirrors.estointernet.in/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz 
        dest: /home/ec2-user
        remote_src: yes
        creates: /home/ec2-user/apache-hive-1.2.2-bin
    - name: entry in bashrc
      blockinfile:
        path: /home/ec2-user/.bashrc
        insertafter: 'fi '
        block: |
            SPARK_HOME=/home/ec2-user/spark-2.4.0-bin-hadoop2.7
            JAVA_HOME=/usr/java/jdk1.8.0
            HIVE_HOME=/home/ec2-user/apache-hive-1.2.2-bin
            HADOOP_HOME=/home/ec2-user/hadoop-2.7.3
            PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
            export PATH
    - name: set java path in hadoop-env.sh file 
      lineinfile:
        path: /home/ec2-user/hadoop-2.7.3/etc/hadoop/hadoop-env.sh
        regexp: '^export JAVA_HOME='
        line: 'export JAVA_HOME=/usr/java/jdk1.8.0'
    - name: to start cluster services
      command: "{{item}}" 
      with_items:
       - hostnamectl set-hostname ec2-13-234-189-125.ap-south-1.compute.amazonaws.com
       - /home/ec2-user/hadoop-2.7.3/bin/hdfs namenode -format -force
       - /home/ec2-user/hadoop-2.7.3/sbin/hadoop-daemon.sh stop namenode
       - /home/ec2-user/hadoop-2.7.3/sbin/hadoop-daemon.sh start namenode
       - /home/ec2-user/hadoop-2.7.3/sbin/hadoop-daemon.sh stop datanode 
       - /home/ec2-user/hadoop-2.7.3/sbin/hadoop-daemon.sh start datanode 
       - /home/ec2-user/hadoop-2.7.3/sbin/yarn-daemon.sh stop resourcemanager
       - /home/ec2-user/hadoop-2.7.3/sbin/yarn-daemon.sh start resourcemanager
       - /home/ec2-user/hadoop-2.7.3/sbin/yarn-daemon.sh stop nodemanager 
       - /home/ec2-user/hadoop-2.7.3/sbin/yarn-daemon.sh start nodemanager 
